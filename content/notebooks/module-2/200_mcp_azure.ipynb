{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='2bcf5774-7819-411d-a606-5394fe77b5a4'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 188, 'total_tokens': 209, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-Cr2hs522e5K7GZL17CQv3hmyfAy7J', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='lc_run--019b5af8-e6e1-7591-aca2-5c1709460940-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': 'call_gohGXAaLUpKkgJVUMQ7qYvZ2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 188, 'output_tokens': 21, 'total_tokens': 209, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchainjs-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchainjs-mcp-adapters: ** THIS REPO ... - GitHub\",\\n      \"content\": \"# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 245 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | RELEASE\\\\\\\\_NOTES.md | RELEASE\\\\\\\\_NOTES.md |  |  |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.\",\\n      \"score\": 0.9999976,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999875,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-adapters/\",\\n      \"title\": \"langchain-mcp-adapters - PyPI\",\\n      \"content\": \"Make Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph agents. * ðŸ› ï¸ Convert MCP tools into LangChain tools that can be used with LangGraph agents. * ðŸ“¦ A client implementation that allows you to connect to multiple MCP servers and load tools from them. Here is a simple example of using the MCP tools with a LangGraph agent. from langchain_mcp_adapters.tools import load_mcp_tools. The library also allows you to connect to multiple MCP servers and load tools from them:. from langchain_mcp_adapters.client import MultiServerMCPClient. > from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.client import MultiServerMCPClient. from langchain_mcp_adapters.client import MultiServerMCPClient. These headers are passed with every HTTP request to the MCP server. from langchain_mcp_adapters.client import MultiServerMCPClient. If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:. from langchain_mcp_adapters.client import MultiServerMCPClient. Details for the file `langchain_mcp_adapters-0.2.1.tar.gz`. Details for the file `langchain_mcp_adapters-0.2.1-py3-none-any.whl`. Hashes for langchain\\\\\\\\_mcp\\\\\\\\_adapters-0.2.1-py3-none-any.whl.\",\\n      \"score\": 0.99998105,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"import{createAgent} from \\\\\"langchain\\\\\"; import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \\\\\"\\\\\") additionalToolNamePrefix \\\\\"\\\\\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Behavior when a server fails to connect: \\\\\"throw\\\\\" (default) or \\\\\"ignore\\\\\" onConnectionError \\\\\"ignore\\\\\",// Server configuration mcpServers{// adds a STDIO connection to a server named \\\\\"math\\\\\" math{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-math\\\\\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-filesystem\\\\\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\\\\\"https://example.com/weather/mcp\\\\\", headers{Authorization \\\\\"Bearer token123\\\\\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\\\\\"oauth-protected-server\\\\\"{url\\\\\"https://protected.example.com/mcp\\\\\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\\\\\"https://myapp.com/oauth/callback\\\\\", clientMetadata{redirect_uris[\\\\\"https://myapp.com/oauth/callback\\\\\"], client_name \\\\\"My MCP Client\\\\\", scope\\\\\"mcp:read mcp:write\\\\\"}}),// Can still include custom headers for non-auth purposes headers{\\\\\"User-Agent\\\\\"\\\\\"My-MCP-Client/1.0\\\\\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \\\\\"sse\\\\\",// also works with \\\\\"type\\\\\" field instead of \\\\\"transport\\\\\" url\\\\\"https://example.com/mcp\\\\\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.\",\\n      \"score\": 0.99995565,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.youtube.com/watch?v=rdvt1qBZJtI\",\\n      \"title\": \"MCP 101: Build your First MCP App with Langchain - YouTube\",\\n      \"content\": \"MCP 101: Build your First MCP App with Langchain\\\\nMarlene Mhangami\\\\n1520 subscribers\\\\n189 likes\\\\n5287 views\\\\n9 Mar 2025\\\\nIn this video I\\'ll walk you through building your first MCP Client application using the MCP Python library and the new langchain-mcp-adapters library and langgraph. We\\'ll connect to the MCP Github server and create an application to help us manage our repositories. \\\\n\\\\nVideo Repository with code (please leave a star): https://github.com/marlenezw/langchain-mcp/tree/main\\\\nMCP Servers Github repository: https://github.com/modelcontextprotocol/servers/tree/main\\\\nLangchain MCP Adapters: https://github.com/langchain-ai/langchain-mcp-adapters\\\\nMCP Docs: https://modelcontextprotocol.info/docs/introduction/\\\\n20 comments\\\\n\",\\n      \"score\": 0.99992394,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"f5a1138f-f7da-4aa3-8960-ab074d76ffee\"\\n}', 'id': 'lc_82bfb618-66b7-4a6c-bbbe-f3e14b73ebcc'}], name='search_web', id='b04e9fc7-6d85-4741-8df6-fcd2c79d1ee3', tool_call_id='call_gohGXAaLUpKkgJVUMQ7qYvZ2', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://github.com/langchain-ai/langchainjs-mcp-adapters', 'title': 'langchain-ai/langchainjs-mcp-adapters: ** THIS REPO ... - GitHub', 'content': '# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\*\\\\* THIS REPO HAS MOVED TO  \\\\*\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 245 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | RELEASE\\\\_NOTES.md | RELEASE\\\\_NOTES.md |  |  |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\*\\\\* THIS REPO HAS MOVED TO  \\\\*\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.', 'score': 0.9999976, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999875, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-adapters/', 'title': 'langchain-mcp-adapters - PyPI', 'content': 'Make Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph agents. * ðŸ› ï¸ Convert MCP tools into LangChain tools that can be used with LangGraph agents. * ðŸ“¦ A client implementation that allows you to connect to multiple MCP servers and load tools from them. Here is a simple example of using the MCP tools with a LangGraph agent. from langchain_mcp_adapters.tools import load_mcp_tools. The library also allows you to connect to multiple MCP servers and load tools from them:. from langchain_mcp_adapters.client import MultiServerMCPClient. > from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.client import MultiServerMCPClient. from langchain_mcp_adapters.client import MultiServerMCPClient. These headers are passed with every HTTP request to the MCP server. from langchain_mcp_adapters.client import MultiServerMCPClient. If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:. from langchain_mcp_adapters.client import MultiServerMCPClient. Details for the file `langchain_mcp_adapters-0.2.1.tar.gz`. Details for the file `langchain_mcp_adapters-0.2.1-py3-none-any.whl`. Hashes for langchain\\\\_mcp\\\\_adapters-0.2.1-py3-none-any.whl.', 'score': 0.99998105, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'import{createAgent} from \"langchain\"; import{ChatOpenAI} from\"@langchain/openai\"; import{MultiServerMCPClient} from\"@langchain/mcp-adapters\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \"\") additionalToolNamePrefix \"\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Behavior when a server fails to connect: \"throw\" (default) or \"ignore\" onConnectionError \"ignore\",// Server configuration mcpServers{// adds a STDIO connection to a server named \"math\" math{transport \"stdio\", command \"npx\", args[\"-y\",\"@modelcontextprotocol/server-math\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \"stdio\", command \"npx\", args[\"-y\",\"@modelcontextprotocol/server-filesystem\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\"https://example.com/weather/mcp\", headers{Authorization \"Bearer token123\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\"oauth-protected-server\"{url\"https://protected.example.com/mcp\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\"https://myapp.com/oauth/callback\", clientMetadata{redirect_uris[\"https://myapp.com/oauth/callback\"], client_name \"My MCP Client\", scope\"mcp:read mcp:write\"}}),// Can still include custom headers for non-auth purposes headers{\"User-Agent\"\"My-MCP-Client/1.0\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \"sse\",// also works with \"type\" field instead of \"transport\" url\"https://example.com/mcp\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.', 'score': 0.99995565, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=rdvt1qBZJtI', 'title': 'MCP 101: Build your First MCP App with Langchain - YouTube', 'content': \"MCP 101: Build your First MCP App with Langchain\\nMarlene Mhangami\\n1520 subscribers\\n189 likes\\n5287 views\\n9 Mar 2025\\nIn this video I'll walk you through building your first MCP Client application using the MCP Python library and the new langchain-mcp-adapters library and langgraph. We'll connect to the MCP Github server and create an application to help us manage our repositories. \\n\\nVideo Repository with code (please leave a star): https://github.com/marlenezw/langchain-mcp/tree/main\\nMCP Servers Github repository: https://github.com/modelcontextprotocol/servers/tree/main\\nLangchain MCP Adapters: https://github.com/langchain-ai/langchain-mcp-adapters\\nMCP Docs: https://modelcontextprotocol.info/docs/introduction/\\n20 comments\\n\", 'score': 0.99992394, 'raw_content': None}], 'response_time': 0.0, 'request_id': 'f5a1138f-f7da-4aa3-8960-ab074d76ffee'}}}),\n",
      "              AIMessage(content=\"The `langchain-mcp-adapters` library is designed to make Anthropic's Model Context Protocol (MCP) tools compatible with LangChain and LangGraph applications. Here are the primary features and details of the library:\\n\\n1. **Tool Conversion**: The library provides functionality to convert MCP tools into LangChain- and LangGraph-compatible tools. This allows you to integrate external MCP tools seamlessly into your LangChain workflows.\\n\\n2. **Server Interactions**:\\n   - It enables connections to multiple MCP servers.\\n   - Provides utilities for loading tools from those servers.\\n   - The client implementation handles communication with MCP servers across different transports such as standard input/output (stdio) and server-sent events (SSE).\\n   \\n3. **Application Integration**: Through this library, agents can combine tools from multiple MCP servers, streamlining the process of interacting with the broader MCP tool ecosystem. \\n\\n4. **Development Details**:\\n   - Created to simplify integration of the growing ecosystem of MCP tool servers with LangChain and LangGraph.\\n   - Useful for scenarios requiring interaction between LangChain environments and tools in MCP-supported ecosystems.\\n\\n5. **Client Features**: The `MultiServerMCPClient` is a key component of the library and allows configuration and authentication (e.g., OAuth 2.0 for secure servers). It also supports features like automatic fallback for certain transport mechanisms and error handling for tool/server connections.\\n\\nIf you want to explore more details or examples, you can refer to the following resources:\\n- [GitHub Repository](https://github.com/langchain-ai/langchainjs-mcp-adapters)\\n- [PyPI Listing](https://pypi.org/project/langchain-mcp-adapters/)\\n- [LangGraph & MCP Changelog](https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph)\\n\\nWould you like to dive deeper into usage examples or setup instructions?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 388, 'prompt_tokens': 2175, 'total_tokens': 2563, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-Cr2hzls6k4zoNgRVnhr2Hau63fzzq', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='lc_run--019b5af9-112f-71a0-87e1-6079fb2fae8c-0', usage_metadata={'input_tokens': 2175, 'output_tokens': 388, 'total_tokens': 2563, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='61b942fb-07b6-424e-87ae-9d2764a0b7d5'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 215, 'total_tokens': 234, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-Cr2jxpOgGf9qvXLuz0IYYWoTlZjfb', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='lc_run--019b5afa-dc75-7ab0-8d1f-ec016435ef7b-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_5oapTWFcAzubpECHf5O8veqX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 215, 'output_tokens': 19, 'total_tokens': 234, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2025-12-26T09:05:54-05:00\",\\n  \"day_of_week\": \"Friday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_e4e8f9f6-b320-426a-9125-25eefda82b15'}], name='get_current_time', id='974706cb-de59-49a4-bdab-87a02ad53800', tool_call_id='call_5oapTWFcAzubpECHf5O8veqX'),\n",
      "              AIMessage(content='The current time is 9:05 AM on Friday, December 26, 2025, in the America/New_York timezone.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 292, 'total_tokens': 322, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-Cr2jzH16HgrpzyjxQBKK37MuJPAh8', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='lc_run--019b5afa-f5a7-72c1-88a3-a30dc93189d7-0', usage_metadata={'input_tokens': 292, 'output_tokens': 30, 'total_tokens': 322, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time is 9:05 AM on Friday, December 26, 2025, in the America/New_York timezone.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285be8c-9b55-4cde-9a3a-470ef0f453ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
